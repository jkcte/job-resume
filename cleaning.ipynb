{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('resume_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['address', 'career_objective', 'skills', 'educational_institution_name',\n",
      "       'degree_names', 'passing_years', 'educational_results', 'result_types',\n",
      "       'major_field_of_studies', 'professional_company_names', 'company_urls',\n",
      "       'start_dates', 'end_dates', 'related_skils_in_job', 'positions',\n",
      "       'locations', 'responsibilities', 'extra_curricular_activity_types',\n",
      "       'extra_curricular_organization_names',\n",
      "       'extra_curricular_organization_links', 'role_positions', 'languages',\n",
      "       'proficiency_levels', 'certification_providers', 'certification_skills',\n",
      "       'online_links', 'issue_dates', 'expiry_dates', 'ï»¿job_position_name',\n",
      "       'educationaL_requirements', 'experiencere_requirement',\n",
      "       'age_requirement', 'responsibilities.1', 'skills_required',\n",
      "       'matched_score'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "UNPACKING PART"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data types of columns\n",
    "categorizedColumns = ['skills', 'educational_institution_name', 'degree_names', 'major_field_of_studies', 'professional_company_names','related_skils_in_job', 'responsibilities', 'extra_curricular_organization_names','extra_curricular_activity_types','certification_skills','responsibilities.1','skills_required']\n",
    "essayColumns = ['career_objective', 'educationaL_requirements']\n",
    "numericalColumns = ['matched_score']\n",
    "dateColumns = ['passing_years','start_dates', 'end_dates','issue_dates', 'expiry_dates','experiencere_requirement']\n",
    "ageColumns = ['age_requirement']\n",
    "#Categorize columns\n",
    "individualInfo = ['address', 'career_objective', 'skills']\n",
    "educationInfo = ['educational_institution_name','degree_names', 'passing_years', 'educational_results', 'result_types','major_field_of_studies']\n",
    "professionalInfo = ['professional_company_names', 'company_urls','start_dates', 'end_dates', 'related_skils_in_job', 'positions','locations', 'responsibilities']\n",
    "extraCurricularInfo = ['extra_curricular_activity_types','extra_curricular_organization_names','extra_curricular_organization_links', 'role_positions']\n",
    "languageInfo = ['language_names', 'language_proficiency_levels']\n",
    "certificationInfo = ['certification_names', 'certification_skills', 'issue_dates', 'expiry_dates']\n",
    "jobInfo = ['job_titles', 'job_company_names', 'job_locations', 'job_start_dates', 'job_end_dates', 'responsibilities.1', 'skills_required', 'educationaL_requirements']\n",
    "similarity = ['matched_score']\n",
    "jobTitleComparison = ['positions', 'role_positions', 'job_titles']\n",
    "responsibilityComparison = ['responsibilities', 'responsibilities.1']\n",
    "educationComparison = ['degree_names', 'major_field_of_studies', 'educationaL_requirements']\n",
    "skillsComparison = ['skills', 'related_skils_in_job', 'skills_required']\n",
    "skillsCertificationComparison = ['skills', 'related_skils_in_job', 'certification_skills']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "#Regex motherfucker\n",
    "# Convert the 'skills' column from string representation of list to actual list\n",
    "df['skills'] = df['skills'].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   Skill  Count\n",
      "0               Big Data    168\n",
      "1                 Hadoop    140\n",
      "2                   Hive    168\n",
      "3                 Python   3612\n",
      "4              Mapreduce     28\n",
      "...                  ...    ...\n",
      "3338        Communicator     28\n",
      "3339       quick learner     28\n",
      "3340       Data Handling     28\n",
      "3341  Customer Analytics     28\n",
      "3342                  MS     28\n",
      "\n",
      "[3343 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Ensure all entries in 'skills' column are lists\n",
    "df['skills'] = df['skills'].apply(lambda x: x if isinstance(x, list) else [])\n",
    "\n",
    "# Flatten the list of skills and count the occurrences\n",
    "skills_counter = Counter(skill for sublist in df['skills'] for skill in sublist)\n",
    "\n",
    "# Convert the counter to a DataFrame for better readability\n",
    "skills_df = pd.DataFrame.from_dict(skills_counter, orient='index', columns=['Count']).reset_index()\n",
    "skills_df.rename(columns={'index': 'Skill'}, inplace=True)\n",
    "\n",
    "print(skills_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the text columns into arrays by splitting on newlines and removing empty strings\n",
    "df['skills_required'] = df['skills_required'].str.split('\\n').apply(lambda x: [item.strip() for item in x] if isinstance(x, list) else [])\n",
    "df['responsibilities.1'] = df['responsibilities.1'].str.split('\\n').apply(lambda x: [item.strip() for item in x] if isinstance(x, list) else [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows removed: 0\n"
     ]
    }
   ],
   "source": [
    "# Get the initial count of rows\n",
    "initial_count = len(df)\n",
    "\n",
    "# Get all columns except matched_score\n",
    "cols_to_check = [col for col in df.columns if col != 'matched_score']\n",
    "\n",
    "# Remove rows where all columns (except matched_score) are empty/null\n",
    "df_cleaned = df.dropna(subset=cols_to_check, how='all')\n",
    "\n",
    "# Calculate number of rows removed\n",
    "rows_removed = initial_count - len(df_cleaned)\n",
    "\n",
    "print(f\"Number of rows removed: {rows_removed}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert all null and missing values to None\n",
    "df = df.where(pd.notnull(df), None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to count elements in array-like strings\n",
    "def count_elements(x):\n",
    "    if isinstance(x, list):\n",
    "        return len(x)\n",
    "    elif isinstance(x, str):\n",
    "        try:\n",
    "            # Try to evaluate string as list\n",
    "            lst = ast.literal_eval(x)\n",
    "            return len(lst)\n",
    "        except:\n",
    "            # If string can't be evaluated as list, split on newlines\n",
    "            return len(x.split('\\n'))\n",
    "    return 0\n",
    "\n",
    "# Add count columns for categorized columns\n",
    "for col in categorizedColumns:\n",
    "    new_col_name = f'{col}_count'\n",
    "    df[new_col_name] = df[col].apply(count_elements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence similarity: 0.16806414072039072\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "from itertools import product\n",
    "\n",
    "# Ensure you have the WordNet data\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "\n",
    "def sentence_similarity(sentence1, sentence2):\n",
    "    # Tokenize sentences\n",
    "    words1 = nltk.word_tokenize(sentence1)\n",
    "    words2 = nltk.word_tokenize(sentence2)\n",
    "    \n",
    "    # Get synsets for each word in the sentences\n",
    "    synsets1 = [wn.synsets(word) for word in words1]\n",
    "    synsets2 = [wn.synsets(word) for word in words2]\n",
    "    \n",
    "    # Filter out empty synsets\n",
    "    synsets1 = [synset[0] for synset in synsets1 if synset]\n",
    "    synsets2 = [synset[0] for synset in synsets2 if synset]\n",
    "    \n",
    "    # Compute similarity between all synset pairs\n",
    "    similarities = []\n",
    "    for synset1, synset2 in product(synsets1, synsets2):\n",
    "        similarity = synset1.path_similarity(synset2)\n",
    "        if similarity is not None:\n",
    "            similarities.append(similarity)\n",
    "    \n",
    "    # Average the similarities\n",
    "    if similarities:\n",
    "        return sum(similarities) / len(similarities)\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "# Example sentences\n",
    "sentence1 = \"Technical Support Troubleshooting Collaboration\"\n",
    "sentence2 = \"Machine Learning Leadership Cross-Functional Collaboration\"\n",
    "\n",
    "# Compute sentence similarity\n",
    "similarity = sentence_similarity(sentence1, sentence2)\n",
    "print(f\"Sentence similarity: {similarity}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "def compute_sentence_similarity(sentence1, sentence2):\n",
    "    # Load pre-trained Sentence-BERT model\n",
    "    model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
    "\n",
    "    # Compute sentence embeddings\n",
    "    embeddings = model.encode([sentence1, sentence2])\n",
    "\n",
    "    # Compute cosine similarity\n",
    "    similarity = util.pytorch_cos_sim(embeddings[0], embeddings[1])\n",
    "    return similarity.item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pathSimilarity(firstSentence, secondSentence, seperator = r'[\\n.]'):\n",
    "    firstSentence = firstSentence.split(seperator)\n",
    "    secondSentence = secondSentence.split(seperator)\n",
    "    similarity = []\n",
    "    setSimilarity = []\n",
    "    for i in firstSentence:\n",
    "        for j in secondSentence:\n",
    "            setSimilarity.append(compute_sentence_similarity(i,j))\n",
    "        similarity.append(max(setSimilarity))\n",
    "        setSimilarity = []\n",
    "    return sum(similarity)/len(similarity)\n",
    "\n",
    "def cosineSimilarity(firstSentence, secondSentence, seperator = r'[\\n.]'):\n",
    "    firstSentence = firstSentence.split(seperator)\n",
    "    secondSentence = secondSentence.split(seperator)\n",
    "    similarity = []\n",
    "    setSimilarity = []\n",
    "    for i in firstSentence:\n",
    "        for j in secondSentence:\n",
    "            setSimilarity.append(sentence_similarity(i,j))\n",
    "        similarity.append(max(setSimilarity))\n",
    "        setSimilarity = []\n",
    "    return sum(similarity)/len(similarity)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 rows of skills_required_count:\n",
      "0    0\n",
      "1    0\n",
      "2    6\n",
      "3    2\n",
      "4    8\n",
      "Name: skills_required_count, dtype: int64\n",
      "\n",
      "First 5 rows of skills_count:\n",
      "0    21\n",
      "1    10\n",
      "2    14\n",
      "3    36\n",
      "4    32\n",
      "Name: skills_count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Count skills in skills_required column and create new feature\n",
    "df['skills_required_count'] = df['skills_required'].apply(lambda x: len(x) if isinstance(x, list) else 0)\n",
    "df['skills_count'] = df['skills'].apply(lambda x: len(x) if isinstance(x, list) else 0)\n",
    "\n",
    "# Print first few rows to verify\n",
    "print(\"First 5 rows of skills_required_count:\")\n",
    "print(df['skills_required_count'].head())\n",
    "print(\"\\nFirst 5 rows of skills_count:\")\n",
    "print(df['skills_count'].head())    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 rows of skills_certification_similarity:\n",
      "0    0.0\n",
      "1    0.0\n",
      "2    0.0\n",
      "3    0.0\n",
      "4    0.0\n",
      "Name: skills_certification_cosine_similarity, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Create similarity score for each row by comparing skills to certification_skills\n",
    "df['skills_certification_path_similarity'] = df.apply(lambda row: \n",
    "    pathSimilarity(\n",
    "        ' '.join(row['skills']) if isinstance(row['skills'], list) else '',\n",
    "        ' '.join(row['certification_skills']) if isinstance(row['certification_skills'], list) else ''\n",
    "    ) if row['skills'] is not None and row['certification_skills'] is not None else 0, \n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Create similarity score for each row by comparing skills to certification_skills\n",
    "df['skills_certification_cosine_similarity'] = df.apply(lambda row: \n",
    "    cosineSimilarity(\n",
    "        ' '.join(row['skills']) if isinstance(row['skills'], list) else '',\n",
    "        ' '.join(row['certification_skills']) if isinstance(row['certification_skills'], list) else ''\n",
    "    ) if row['skills'] is not None and row['certification_skills'] is not None else 0, \n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Print first few rows to verify\n",
    "print(\"First 5 rows of skills_certification_similarity:\")\n",
    "print(df['skills_certification_cosine_similarity'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create similarity score for each row by comparing skills to certification_skills\n",
    "df['skills_certification_path_similarity'] = df.apply(lambda row: \n",
    "    pathSimilarity(\n",
    "        ' '.join(row['skills_related']) if isinstance(row['skills'], list) else '',\n",
    "        ' '.join(row['certification_skills']) if isinstance(row['certification_skills'], list) else ''\n",
    "    ) if row['skills'] is not None and row['certification_skills'] is not None else 0, \n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Create similarity score for each row by comparing skills to certification_skills\n",
    "df['skills_certification_cosine_similarity'] = df.apply(lambda row: \n",
    "    cosineSimilarity(\n",
    "        ' '.join(row['skills_related']) if isinstance(row['skills'], list) else '',\n",
    "        ' '.join(row['certification_skills']) if isinstance(row['certification_skills'], list) else ''\n",
    "    ) if row['skills'] is not None and row['certification_skills'] is not None else 0, \n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Print first few rows to verify\n",
    "print(\"First 5 rows of skills_certification_similarity:\")\n",
    "print(df['skills_certification_similarity'].head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
